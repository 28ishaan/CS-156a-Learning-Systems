{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Nonlinear Transforms"
      ],
      "metadata": {
        "id": "YxXNmp0WRcnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **e**. The polynomial transform of order $Q = 10$ consists of all ${x_1}^i{x_2}^j$ where $i + j \\leq Q$. Let us consider small examples for $Q = \\{1,2,3\\}$.\n",
        "\n",
        "For Q = 1, we have $(1, {x_1}, {x_2})$. $\\mathbb{Z}$ has dimensionality 2.\n",
        "\n",
        "For Q = 2, we have $(1, {x_1}, {x_2}, {x_1}{x_2}, {x_2}^2, {x_1}^2)$. $\\mathbb{Z}$ has dimensionality 5.\n",
        "\n",
        "For Q = 3, we have $(1, {x_1}, {x_2}, {x_1}{x_2}, {x_1}^2, {x_1}^2, {x_1}{x_2}^2, {x_1}^2{x_2})$. $\\mathbb{Z}$ has dimensionality 7.\n",
        "\n",
        "Therefore, the dimensionality of the $\\mathbb{Z}$ space is given by $\\mathbb{Z} = (\\sum_{i=0}^{Q} (Q-i) + 1) - 1$. When $Q = 10$, the dimension of $\\mathbb{Z}$ is 65 which is none of the options."
      ],
      "metadata": {
        "id": "xzMyC1d9SXTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias and Variance"
      ],
      "metadata": {
        "id": "SE4X4rorRh5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **d**. Let us go through all possibilities.\n",
        "\n",
        "For (a), if $H$ has one hypothesis, then $g^D$ will always be the same. The average hypothesis, $\\bar{g}$, will also be the same hypothesis $\\in H$. Therefore, this does not work.\n",
        "\n",
        "For (b), if $H$ is the set of all constant, real-values hypotheses, then each hypothesis, $g^D$, will be a constant, real value. The average hypothesis,$\\bar{g}$, will also be a constant, real value, meaning $\\bar{g} \\in H$. Therefore, this does not work.\n",
        "\n",
        "For (c), if $H$ is the linear regression model, then $g^D$ will be a polynomial with real coefficients. The avearge hypothesis, $\\bar{g}$, will also contain real coefficients, and as a result, $\\bar{g}$ \\in H$. Thus, this also does not work.\n",
        "\n",
        "For (d), if $H$ is the logistic regression model, it relies on a sigmoid function that ranges from 0 to 1. When trying to obtain an average hypothesis, $\\bar{g}$, it may not be a sigmoid function. Thus, $\\bar{g}$ ∉ H$ always."
      ],
      "metadata": {
        "id": "nv4vH1WGSf3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfitting"
      ],
      "metadata": {
        "id": "zCDXvfk6RlPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **d**. Overfitting occurs when a model learns the training data too well, including its noise and outliers, to the point that it performs poorly on new, unseen data. While comparing the values of E_out and E_in can sometimes give insights into overfitting, it's not a foolproof method. There are situations where E_out and E_in might be close or have a small difference, yet the model is still overfitting. To assess overfitting more accurately, we would use techniques such as cross-validation, where the dataset is split into multiple folds for training and testing, or to monitor the performance of the model on a separate validation set that was not used during training."
      ],
      "metadata": {
        "id": "0nJdWp3ISlAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **d**. The statement, Stochastic noise does not depend on the hypothesis, is true. Deterministic noise depends on the complexity of the hypothesis, but stochastic noise is noise related with the data itself."
      ],
      "metadata": {
        "id": "xTHDR0F_Smo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization"
      ],
      "metadata": {
        "id": "9xqV0SlkRuME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **a**. If we are given that $w_{lin}^TΓ^TΓw_{lin} \\leq C$, where $w_{lin}$ is the linear regression solution, then the value for $w_{reg}$ can just be set to this solution."
      ],
      "metadata": {
        "id": "TCE7w9QmSqFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **b**. From lecture, we know that soft order constraints that regularize polynomials minimizes $E_{aug}$. Thus, they can be translated into augmented error."
      ],
      "metadata": {
        "id": "wsA2hG2cStrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularized Linear Regression"
      ],
      "metadata": {
        "id": "IBykxXSNRw0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_file = np.loadtxt('features.train')\n",
        "test_file = np.loadtxt('features.test')\n",
        "\n",
        "x_train = train_file[:, 1:]\n",
        "y_train = train_file[:,0]\n",
        "\n",
        "x_test = test_file[:, 1:]\n",
        "y_test = test_file[:,0]"
      ],
      "metadata": {
        "id": "PLgeauAUyTGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_class(y, n):\n",
        "  bin_class = -np.ones(len(y))\n",
        "  bin_class[y == n] = 1\n",
        "  return bin_class"
      ],
      "metadata": {
        "id": "-htyOkShyqd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_1(x):\n",
        "  transform = np.ones((x.shape[0],1))\n",
        "  return np.hstack((transform, x))"
      ],
      "metadata": {
        "id": "I4t2jnNPzJAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def suedo_regression(x_train, lambd):\n",
        "  val1 = np.dot(x_train.T, x_train)\n",
        "  val2 = lambd * np.identity(x_train.shape[1])\n",
        "  return np.dot(np.linalg.inv(val1 + val2), x_train.T)\n",
        "\n",
        "def calc_error(w, x_, y_):\n",
        "  N = 0\n",
        "  for i in range(x_.shape[0]):\n",
        "    N += max(0, np.sign(-np.dot(w.T, x_[i])*y_[i]))\n",
        "  return N/float(x_.shape[0])"
      ],
      "metadata": {
        "id": "gUNi1kMX1XoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambd = 1.0\n",
        "z_train = transform_1(x_train)\n",
        "for n in [5, 6, 7, 8, 9]:\n",
        "  yt_train = binary_class(y_train, n)\n",
        "  suedo_reg = suedo_regression(z_train, lambd)\n",
        "  w_reg = np.dot(suedo_reg, yt_train)\n",
        "  err_train_reg = calc_error(w_reg, z_train, yt_train)\n",
        "  print(f'{n} versus all: E_in = {err_train_reg}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QevvWfla1j2A",
        "outputId": "e710f3a1-cb61-4fde-c674-b4ccd5c386f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 versus all: E_in = 0.07625840076807022\n",
            "6 versus all: E_in = 0.09107118365107666\n",
            "7 versus all: E_in = 0.08846523110684405\n",
            "8 versus all: E_in = 0.07433822520916199\n",
            "9 versus all: E_in = 0.08832807570977919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **d**. 8 versus all has the lowest."
      ],
      "metadata": {
        "id": "Vwk1JG9mSyuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_2(x):\n",
        "  x_t = np.ones((x.shape[0],1))\n",
        "  return np.hstack((x_t, x[:,[0]],x[:,[1]], x[:,[1]]*x[:,[0]], x[:,[0]]**2, x[:,[1]]**2))"
      ],
      "metadata": {
        "id": "Rqwx6xue2fo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambd = 1.0\n",
        "z_train = transform_2(x_train)\n",
        "z_test = transform_2(x_test)\n",
        "for n in [0,1,2,3,4]:\n",
        "  yt_train = binary_class(y_train, n)\n",
        "  yt_test = binary_class(y_test, n)\n",
        "  suedo_reg = suedo_regression(z_train, lambd)\n",
        "  w_reg = np.dot(suedo_reg, yt_train)\n",
        "  err_test_reg = calc_error(w_reg, z_test, yt_test)\n",
        "  print(f'{n} versus all: E_in = {err_test_reg}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq6_Z87l24Eh",
        "outputId": "1c11840a-836e-4b8c-d100-80ad1ea765e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 versus all: E_in = 0.10662680617837568\n",
            "1 versus all: E_in = 0.02192326856003986\n",
            "2 versus all: E_in = 0.09865470852017937\n",
            "3 versus all: E_in = 0.08271051320378675\n",
            "4 versus all: E_in = 0.09965122072745392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **b**. 1 versus all has the lowest."
      ],
      "metadata": {
        "id": "GalZBBt9Sy2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambd = 1.0\n",
        "\n",
        "xt_train = transform_1(x_train)\n",
        "xt_test = transform_1(x_test)\n",
        "\n",
        "z_train = transform_2(x_train)\n",
        "z_test = transform_2(x_test)\n",
        "\n",
        "for n in range(10):\n",
        "  yt_train = binary_class(y_train, n)\n",
        "  yt_test = binary_class(y_test, n)\n",
        "\n",
        "  x_reg = suedo_regression(xt_train, lambd)\n",
        "  w_reg_x = np.dot(x_reg, yt_train)\n",
        "\n",
        "  z_reg = suedo_regression(z_train, lambd)\n",
        "  w_reg_z = np.dot(z_reg, yt_train)\n",
        "\n",
        "  xtrain_error = calc_error(w_reg_x, xt_train, yt_train)\n",
        "  ztrain_error = calc_error(w_reg_z, z_train, yt_train)\n",
        "\n",
        "  xtest_error = calc_error(w_reg_x, xt_test, yt_test)\n",
        "  ztest_error = calc_error(w_reg_z, z_test, yt_test)\n",
        "\n",
        "  diff = float(xtest_error - ztest_error)\n",
        "  print(f'{n} versus all improvement = {diff}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3IiEL4w4I-r",
        "outputId": "6f0a5eb4-4c55-423a-91d7-c1d569ffa84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 versus all improvement = 0.008470353761833582\n",
            "1 versus all improvement = 0.0004982561036372679\n",
            "2 versus all improvement = 0.0\n",
            "3 versus all improvement = 0.0\n",
            "4 versus all improvement = 0.0\n",
            "5 versus all improvement = 0.0004982561036372679\n",
            "6 versus all improvement = 0.0\n",
            "7 versus all improvement = 0.0\n",
            "8 versus all improvement = 0.0\n",
            "9 versus all improvement = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **e**. As shown in the output, the out of sample Error improves for 5 versus all, but by less than 5%."
      ],
      "metadata": {
        "id": "wQeAy3YQSy_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zc_train = transform_2(x_train[np.logical_or(y_train==1, y_train==5), :])\n",
        "yc_train = binary_class(y_train[np.logical_or(y_train==1, y_train==5)], 1)\n",
        "zc_test = transform_2(x_test[np.logical_or(y_test==1, y_test==5), :])\n",
        "yc_test = binary_class(y_test[np.logical_or(y_test==1, y_test==5)], 1)\n",
        "\n",
        "for lambd in [0.01, 1.0]:\n",
        "  suedo_reg = suedo_regression(zc_train, lambd)\n",
        "  w_reg = np.dot(suedo_reg, yc_train)\n",
        "\n",
        "  err_train_reg = calc_error(w_reg, zc_train, yc_train)\n",
        "  err_test_reg = calc_error(w_reg, zc_test, yc_test)\n",
        "\n",
        "  print(f'lambda = {lambd}: E_in: {err_train_reg}, E_out: {err_test_reg}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6ZaIbOaMeWL",
        "outputId": "2689efff-b4a2-4684-e43e-6081799b429d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda = 0.01: E_in: 0.004484304932735426, E_out: 0.02830188679245283\n",
            "lambda = 1.0: E_in: 0.005124919923126201, E_out: 0.025943396226415096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **a**. Overfitting occurs."
      ],
      "metadata": {
        "id": "k-vJCzf4SzHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines"
      ],
      "metadata": {
        "id": "gtkzeBiiR2HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1,0],[0,1],[0,-1],[-1,0],[0,2],[0,-2],[-2,0]])\n",
        "y = np.array([-1, -1, -1, 1, 1, 1, 1])\n",
        "\n",
        "def z_transform(x_i):\n",
        "  return [x_i[1]**2 - 2 * x_i[0] - 1, x_i[0]**2 - 2 * x_i[1] + 1]\n",
        "\n",
        "z = np.apply_along_axis(z_transform, 1, x)"
      ],
      "metadata": {
        "id": "Zuq6QGKxesTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. **c**. Plotting the points, we see that $z_2$ does not make any difference to the classification. This means that $w_2$ is 0. Using geometry we find $w_1 = 1$ and b = -0.5."
      ],
      "metadata": {
        "id": "QfJj4ubCS_wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install qpsolvers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3geWRXVAwbd",
        "outputId": "470da03d-3342-4fd7-8687-c3bd16a648de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qpsolvers in /usr/local/lib/python3.10/dist-packages (4.0.1)\n",
            "Requirement already satisfied: daqp>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from qpsolvers) (0.5.1)\n",
            "Requirement already satisfied: ecos>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from qpsolvers) (2.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from qpsolvers) (1.23.5)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from qpsolvers) (0.6.2.post8)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from qpsolvers) (1.11.3)\n",
            "Requirement already satisfied: scs>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from qpsolvers) (3.2.4)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.6.2->qpsolvers) (0.1.7.post0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from qpsolvers import solve_qp\n",
        "\n",
        "def get_svms_quadp(x, y):\n",
        "  y = y.reshape(-1, 1)\n",
        "  m = x.shape[0]\n",
        "  K = ((np.dot(x, x.T) + 1) ** 2).astype(float)\n",
        "\n",
        "  P = (np.outer(y, y) * K).astype(float)\n",
        "  q = -np.ones((m, 1), dtype=float)\n",
        "  G = -np.eye(m, dtype=float)\n",
        "  h = np.zeros((m, 1), dtype=float)\n",
        "  A = y.T.astype(float)\n",
        "  b = np.zeros((1, 1), dtype=float)\n",
        "\n",
        "  alpha = solve_qp(P, q, G, h, A, b, solver=\"cvxopt\")\n",
        "  support_vector_indices = np.where(alpha > 1e-5)[0]\n",
        "  support_vectors = z[support_vector_indices]\n",
        "  return len(support_vector_indices)\n",
        "\n",
        "num_svms = get_svms_quadp(x, y)\n",
        "print(f'SVMs: {num_svms}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKA3bvCvfgym",
        "outputId": "f07a8320-e628-42a7-ab76-aa44dda2d9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVMs: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **c**"
      ],
      "metadata": {
        "id": "rFmyUuvDTAkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Radial Basis Functions"
      ],
      "metadata": {
        "id": "P0dmtiKpR7IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from sklearn import svm\n",
        "\n",
        "def func(x_i):\n",
        "  return np.sign(x_i[1] - x_i[0] + 0.25*np.sin(np.pi*x_i[0]))\n",
        "\n",
        "def points(n):\n",
        "  x = np.array([[random.uniform(-1,1),random.uniform(-1,1)] for i in range(n)])\n",
        "  y = []\n",
        "  for i in x:\n",
        "    y.append(func(i))\n",
        "  return x, np.array(y)\n",
        "\n",
        "def get_freq():\n",
        "  clf = svm.SVC(kernel='rbf', coef0=1, C=1e10, gamma=1.5)\n",
        "  fail_score = 0\n",
        "  for i in range(1000):\n",
        "    x,y = points(100)\n",
        "    clf.fit(x,y)\n",
        "    E_in = 1.0 - clf.score(x,y)\n",
        "    if E_in != 0:\n",
        "      fail_score += 1\n",
        "  result = fail_score/float(1000)\n",
        "  return result\n",
        "\n",
        "frequency = get_freq()\n",
        "print(f'Frequency: {frequency}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i2BPYO9fuAh",
        "outputId": "bcb2bb01-6978-4cbc-92a8-00894b0ab423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. **a**"
      ],
      "metadata": {
        "id": "MHfidYdxTGNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "G_fiHDq7XLZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rbf_model(x, center, gamma):\n",
        "  return np.exp(-gamma * np.sum((x - center)**2))\n",
        "\n",
        "def rbf_reg(x, centers, gamma):\n",
        "  z = np.empty((x.shape[0], centers.shape[0] + 1))\n",
        "  z[:,0] = np.ones(x.shape[0])\n",
        "  for i in range(centers.shape[0]):\n",
        "    z[:, i+1] = np.apply_along_axis(rbf_model, 1, x, centers[i], gamma)\n",
        "  return z\n",
        "\n",
        "def calc_w(z, y):\n",
        "  m = np.matrix(np.dot(z.T, z))\n",
        "  b = np.dot(np.dot(m.getI(), z.T), y)\n",
        "  return b.getA()[0,:]"
      ],
      "metadata": {
        "id": "rpZlPzZ7h9gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_freq():\n",
        "  gamma = 1.5\n",
        "  K = 9\n",
        "  n = 100\n",
        "  ker_Eout = []\n",
        "  reg_Eout = []\n",
        "  for i in range(100):\n",
        "    x, y = points(n)\n",
        "    x_test, y_test = points(1000)\n",
        "    clf = svm.SVC(kernel='rbf', coef0=1, C=1e10, gamma=gamma)\n",
        "    clf.fit(x,y)\n",
        "    while clf.score(x,y) != 1.0:\n",
        "      x,y = points(n)\n",
        "      clf.fit(x,y)\n",
        "\n",
        "    ker_Eout.append(1.0 - clf.score(x_test,y_test))\n",
        "    kmeans = KMeans(n_clusters=K, random_state=0, n_init=\"auto\").fit(x)\n",
        "    kmeans_centers = kmeans.cluster_centers_\n",
        "\n",
        "    z = rbf_reg(x, kmeans_centers, gamma)\n",
        "    w = calc_w(z, y)\n",
        "    z_test = rbf_reg(x_test, kmeans_centers, gamma)\n",
        "    y_ = np.dot(z_test, w)\n",
        "    reg_Eout.append(np.sum(y_*y_test < 0) / float(y_test.shape[0]))\n",
        "\n",
        "  diff = np.array(ker_Eout) - np.array(reg_Eout)\n",
        "  result = sum(i < 0 for i in diff)/float(len(diff))\n",
        "  return result\n",
        "\n",
        "frequency = get_freq()\n",
        "print(f'Frequency: {frequency}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu5zlld5iQqK",
        "outputId": "5f02039d-f4ab-406d-da27-9fde17a5d15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. **e**"
      ],
      "metadata": {
        "id": "f05ismAiTHZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_freq():\n",
        "  gamma = 1.5\n",
        "  K = 12\n",
        "  n = 100\n",
        "  ker_Eout = []\n",
        "  reg_Eout = []\n",
        "  for i in range(100):\n",
        "    x, y = points(n)\n",
        "    x_test, y_test = points(1000)\n",
        "    clf = svm.SVC(kernel='rbf', coef0=1, C=1e10, gamma=gamma)\n",
        "    clf.fit(x,y)\n",
        "    while clf.score(x,y) != 1.0:\n",
        "      x,y = points(n)\n",
        "      clf.fit(x,y)\n",
        "    ker_Eout.append(1.0 - clf.score(x_test,y_test))\n",
        "    kmeans = KMeans(n_clusters=K, random_state=0, n_init=\"auto\").fit(x)\n",
        "    kmeans_centers = kmeans.cluster_centers_\n",
        "\n",
        "    z = rbf_reg(x, kmeans_centers, gamma)\n",
        "    w = calc_w(z, y)\n",
        "    z_test = rbf_reg(x_test, kmeans_centers, gamma)\n",
        "    y_ = np.dot(z_test, w)\n",
        "    reg_Eout.append(np.sum(y_*y_test < 0) / float(y_test.shape[0]))\n",
        "\n",
        "  diff = np.array(ker_Eout) - np.array(reg_Eout)\n",
        "  result = sum(i < 0 for i in diff)/float(len(diff))\n",
        "  return result\n",
        "\n",
        "frequency = get_freq()\n",
        "print(f'Frequency: {frequency}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rq8HQ7pkjex",
        "outputId": "393f7c6e-08b8-4638-c500-d18558e83180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. **d**"
      ],
      "metadata": {
        "id": "hRzMSa-sTKsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_freq():\n",
        "  gamma = 1.5\n",
        "  reg_Ein = []\n",
        "  reg_Eout = []\n",
        "  Ks = [9, 12]\n",
        "\n",
        "  for K in Ks:\n",
        "    for i in range(100):\n",
        "      x,y = points(100)\n",
        "      x_test, y_test = points(1000)\n",
        "      kmeans = KMeans(n_clusters=K, random_state=0, n_init=\"auto\").fit(x)\n",
        "      kmeans_centers = kmeans.cluster_centers_\n",
        "      z = rbf_reg(x, kmeans_centers, gamma)\n",
        "      w = calc_w(z, y)\n",
        "      y_ = np.dot(z, w)\n",
        "      reg_Ein.append(np.sum(y_*y < 0) / float(y_test.shape[0]))\n",
        "      z_test = rbf_reg(x_test, kmeans_centers, gamma)\n",
        "      y_ = np.dot(z_test, w)\n",
        "      reg_Eout.append(np.sum(y_*y_test < 0) / float(y_test.shape[0]))\n",
        "\n",
        "  reg_Ein = np.array(reg_Ein).reshape(2,100).T\n",
        "  reg_Eout = np.array(reg_Eout).reshape(2,100).T\n",
        "\n",
        "  return reg_Ein, reg_Eout\n",
        "\n",
        "reg_Ein, reg_Eout = get_freq()\n",
        "\n",
        "# a. E_in goes down but E_out goes up\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] > 0) & (reg_Eout[:,0]-reg_Eout[:,1] < 0))\n",
        "print(f'E_in down, E_out up: {result}')\n",
        "\n",
        "# b. E_in goes up but E_out goes down\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] < 0) & (reg_Eout[:,0]-reg_Eout[:,1] > 0))\n",
        "print(f'E_in up, E_out down: {result}')\n",
        "\n",
        "# c. Both E_in and E_out go up\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] < 0) & (reg_Eout[ :,0]-reg_Eout[:,1] < 0))\n",
        "print(f'E_in, E_out up: {result}')\n",
        "\n",
        "# d. Both E_in and E_out go down\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] > 0) & (reg_Eout [:,0]-reg_Eout[:,1] > 0))\n",
        "print(f'E_in, E_out down: {result}')\n",
        "\n",
        "# e. E_in and E_out remain the same\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] == 0) & ( reg_Eout[:,0]-reg_Eout[:,1] == 0))\n",
        "print(f'E_in, E_out remain same: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo3VjMsWk3iD",
        "outputId": "2ad5d0b6-7777-4bae-fb88-fd3f3196425c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E_in down, E_out up: 11\n",
            "E_in up, E_out down: 12\n",
            "E_in, E_out up: 10\n",
            "E_in, E_out down: 50\n",
            "E_in, E_out remain same: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. **d**"
      ],
      "metadata": {
        "id": "2A2ysty0TN0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = 9\n",
        "reg_Ein = []\n",
        "reg_Eout = []\n",
        "gammas = [1.5, 2]\n",
        "\n",
        "for gamma in gammas:\n",
        "  for i in range(100):\n",
        "    x,y = points(100)\n",
        "    x_test, y_test = points(1000)\n",
        "    kmeans = KMeans(n_clusters=K, random_state=0, n_init=\"auto\").fit(x)\n",
        "    kmeans_centers = kmeans.cluster_centers_\n",
        "    z = rbf_reg(x, kmeans_centers, gamma)\n",
        "    w = calc_w(z, y)\n",
        "    y_ = np.dot(z, w)\n",
        "    reg_Ein.append(np.sum(y_*y < 0) / float(y_test.shape[0]))\n",
        "    z_test = rbf_reg(x_test, kmeans_centers, gamma)\n",
        "    y_ = np.dot(z_test, w)\n",
        "    reg_Eout.append(np.sum(y_*y_test < 0) / float(y_test.shape[0]))\n",
        "\n",
        "reg_Ein = np.array(reg_Ein).reshape(2,100).T\n",
        "reg_Eout = np.array(reg_Eout).reshape(2,100).T\n",
        "\n",
        "# a. E_in goes down but E_out goes up\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] > 0) & (reg_Eout[:,0]-reg_Eout[:,1] < 0))\n",
        "print(f'E_in down, E_out up: {result}')\n",
        "\n",
        "# b. E_in goes up but E_out goes down\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] < 0) & (reg_Eout[:,0]-reg_Eout[:,1] > 0))\n",
        "print(f'E_in up, E_out down: {result}')\n",
        "\n",
        "# c. Both E_in and E_out go up\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] < 0) & (reg_Eout[ :,0]-reg_Eout[:,1] < 0))\n",
        "print(f'E_in, E_out up: {result}')\n",
        "\n",
        "# d. Both E_in and E_out go down\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] > 0) & (reg_Eout [:,0]-reg_Eout[:,1] > 0))\n",
        "print(f'E_in, E_out down: {result}')\n",
        "\n",
        "# e. E_in and E_out remain the same\n",
        "result = np.sum((reg_Ein[:,0]-reg_Ein[:,1] == 0) & ( reg_Eout[:,0]-reg_Eout[:,1] == 0))\n",
        "print(f'E_in, E_out remain same: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlookuiJmwZS",
        "outputId": "521e504b-3a60-4bdf-afdd-48054dadcb93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E_in down, E_out up: 24\n",
            "E_in up, E_out down: 20\n",
            "E_in, E_out up: 27\n",
            "E_in, E_out down: 15\n",
            "E_in, E_out remain same: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **c**"
      ],
      "metadata": {
        "id": "dKop76E5TTn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_freq():\n",
        "  K = 9\n",
        "  n = 100\n",
        "  reg_Ein = []\n",
        "  for i in range(100):\n",
        "    x, y = points(n)\n",
        "    kmeans = KMeans(n_clusters=K, random_state=0, n_init=\"auto\").fit(x)\n",
        "    kmeans_centers = kmeans.cluster_centers_\n",
        "    z = rbf_reg(x, kmeans_centers, 1.5)\n",
        "    w = calc_w(z, y)\n",
        "    y_ = np.dot(z, w)\n",
        "    reg_Ein.append(np.sum(y_*y < 0) / float(y_test.shape[0]))\n",
        "\n",
        "  result = sum(E_in == 0 for E_in in reg_Ein)/float(len(reg_Ein))\n",
        "  return result\n",
        "\n",
        "frequency = get_freq()\n",
        "print(f'Frequency: {frequency}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GULvU6ene-y",
        "outputId": "e7dc6b0b-9d42-4bea-e182-16cf2f2eb26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. **a**"
      ],
      "metadata": {
        "id": "Ye7DGyXPTVKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Priors"
      ],
      "metadata": {
        "id": "dcPS1MT8SAEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. **b**. We are assuming the prior that $P(h=f)$ is uniform over $f \\in [0, 1]$. We also know that the posterior is directly proportional to the likelihood times the prior. The likelihood increases linearly over the interval [0,1] since it is given that f is a constant over this interval. Therefore, we know that the posterior also increases linearly over [0,1]."
      ],
      "metadata": {
        "id": "T-gs0DgtTX28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregation"
      ],
      "metadata": {
        "id": "XJbNk19RSGma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. **c**. If given two learned hypotheses, $g_1$ and  $g_2$, and construct the average hypothesis g defined by $g(x) = \\frac{1}{2}(g_1(x) + g_2(x))$ for all x, it means that both $g_1$ and $g_2$ contribute the same amount. The Mean Squared Error takes the sum of the difference squared. It follows that the average of the hypotheses residuals squared will be higher than the square of the average of the residuals. Thus, we know that $E_{out}(g)$ cannot be worse than the average of $E_{out}(g_1)$ and $E_{out}(g_2)$."
      ],
      "metadata": {
        "id": "aEL--q6zSKrN"
      }
    }
  ]
}