{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Error"
      ],
      "metadata": {
        "id": "fvZ6DXKKAJHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (c) 100"
      ],
      "metadata": {
        "id": "DZPPFoLQBlwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathbb{E}_D[E_{in}(w_{lin})] = σ^2(1-\\frac{d+1}{N})$.\n",
        "For $σ = 0.1 and d = 8$, we have $\\mathbb{E}_D[E_{in}(w_{lin})] = (0.1)^2(1-\\frac{9}{N})$. Of the choices for N, **[c] 100** is the smallest N that results in an expected $E_{in}$ greater than 0.008."
      ],
      "metadata": {
        "id": "pBNHG_zbAW6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. (d) $w_1 < 0, w_2 > 0$"
      ],
      "metadata": {
        "id": "kakGeULmBonX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sign($w^T \\cdot x$) = sign($w_0 \\cdot 1 + w_1 \\cdot x_1 + w_2 \\cdot x_2$). When $w_1$ is negative and $w_2$ is positive, the expression can express the hyperbolic boundary. For example, when $|w_2|$ is very large and $|w_1|$ is not large, we get positive values and the opposite results in negative values. This is the hyperbolic boundary displayed."
      ],
      "metadata": {
        "id": "jUnqFH7DFO7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. (c) 15"
      ],
      "metadata": {
        "id": "87l6XmvxB3t4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know $d_{vc} \\leq d + 1$. Since the given input space has 14 parameters, **[c] 15** is the smallest answer choice not smaller than $d_{vc} \\leq 15$."
      ],
      "metadata": {
        "id": "TTfxrT3lI-TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent"
      ],
      "metadata": {
        "id": "j1CKtvt9vN1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. (e) $2(e^v + 2ve^{-u})(ue^v - 2ve^{-u})$"
      ],
      "metadata": {
        "id": "xzrZf7k1B94n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$E(u, v) = (ue^v - 2ve^{-u})^2$\n",
        "\n",
        "$\\frac{∂E}{∂u} = 2(ue^v - 2ve^{-u})(e^v+2ve^{-u})$"
      ],
      "metadata": {
        "id": "vN-q280RvWU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. (d) 10 (see code)"
      ],
      "metadata": {
        "id": "i8zhMYQnCTRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. (e) (0.045, 0.024) (see code)"
      ],
      "metadata": {
        "id": "LjVmSd7JCUSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def partial_u(u,v):\n",
        "\treturn 2 * (u * math.exp(v) - 2 * v * math.exp(-u)) * (math.exp(v) + 2 * v * math.exp(-u))\n",
        "\n",
        "def partial_v(u,v):\n",
        "\treturn 2 * (u * math.exp(v) - 2 * v * math.exp(-u)) * (u * math.exp(v) - 2 * math.exp(-u))\n",
        "\n",
        "def E(u,v):\n",
        "\treturn (u * math.exp(v) - 2 * v * math.exp(-u)) ** 2\n",
        "\n",
        "def gradient(u, v, lr, threshold):\n",
        "  iterations = 0\n",
        "  error = E(u, v)\n",
        "  while(error > threshold):\n",
        "    du = partial_u(u, v)\n",
        "    dv = partial_v(u, v)\n",
        "    u = u - du * lr\n",
        "    v = v - dv * lr\n",
        "    error = E(u, v)\n",
        "    iterations += 1\n",
        "  return (iterations, u, v)\n",
        "\n",
        "res = gradient(1, 1, 0.1, 10**(-14))\n",
        "print(f'Iterations: {res[0]}\\nu: {res[1]}\\nv: {res[2]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfVPAZ85wuva",
        "outputId": "210260a6-4600-4a27-dc7c-8965f2bde053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations: 10\n",
            "u: 0.04473629039778207\n",
            "v: 0.023958714099141746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. (a) $10^{-1}$"
      ],
      "metadata": {
        "id": "pzPxg_bECYy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient2(u, v, lr, max_iterations):\n",
        "  iterations = 0\n",
        "  while(iterations < max_iterations):\n",
        "    du = partial_u(u, v)\n",
        "    u = u - du * lr\n",
        "    dv = partial_v(u, v)\n",
        "    v = v - dv * lr\n",
        "    iterations += 1\n",
        "  error2 = E(u, v)\n",
        "  return (error2)\n",
        "\n",
        "error2 = gradient2(1, 1, 0.1, 15)\n",
        "print(f'Error after 15 iterations: {error2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K50Zf2K3tgj",
        "outputId": "f8e84cb8-e1ab-4499-fef3-7c42a40bfea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error after 15 iterations: 0.13981379199615315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "ch1Sfu3i5UWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. (d) 0.100"
      ],
      "metadata": {
        "id": "DuSQM4ULCdZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. (a) 350"
      ],
      "metadata": {
        "id": "UVs9By5FChwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rnd\n",
        "import numpy as np\n",
        "\n",
        "def gen_line():\n",
        "    [x1,x2,y1,y2] = [rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0)]\n",
        "    w = np.array([x2*y1-y2*x1, y2-y1, x1-x2])\n",
        "    return w, [x1,x2,y1,y2]\n",
        "\n",
        "def gen_pts(n, d, w=None):\n",
        "    if w is None:\n",
        "        w, li = gen_line()\n",
        "\n",
        "    d_ = np.random.uniform(-1.0, 1.0,(d,n))\n",
        "    x_ = np.append(np.ones(n), d_).reshape((d+1,n))\n",
        "    y = np.sign(np.dot(w.T,x_))\n",
        "    d_ = np.append(x_, y).reshape((d+2,n))\n",
        "\n",
        "    return w, d_\n",
        "\n",
        "def compute_gradient(w, x_n, y_n):\n",
        "    return -y_n*x_n/(1+np.exp(y_n*np.dot(w.T,x_n)))\n",
        "\n",
        "def update(w, d_, eta, rand_perm):\n",
        "    for n in rand_perm:\n",
        "        x_n = np.array([d_[0][n], d_[1][n], d_[2][n]])\n",
        "        y_n = np.array([d_[3][n]])\n",
        "\n",
        "        v_t = -compute_gradient(w, x_n, y_n)\n",
        "        w = w + eta * v_t\n",
        "\n",
        "    return w\n",
        "\n",
        "def calc_error(w, d_):\n",
        "    return np.sum(np.log((1+np.exp(-d_[3]*np.dot(w.T,d_[0:3])))))/ len(d_[0])\n",
        "\n",
        "eta = 0.01\n",
        "\n",
        "E_out_list = []\n",
        "epoch_list = []\n",
        "\n",
        "for i in range(100):\n",
        "    w, d_ = gen_pts(100,2)\n",
        "    _, d_test = gen_pts(5000, 2, w=w)\n",
        "    w_init = np.array([0.0, 0.0, 0.0])\n",
        "    w_prior = w_init\n",
        "    rand_perm = np.random.permutation(len(d_[0]))\n",
        "    w_ = update(w_prior, d_, eta, rand_perm)\n",
        "    epoch = 1\n",
        "\n",
        "    while np.linalg.norm(w_prior - w_) >= 0.01:\n",
        "        w_prior = w_\n",
        "        rand_perm = np.random.permutation(len(d_[0]))\n",
        "        w_ = update(w_prior, d_, eta, rand_perm)\n",
        "        epoch += 1\n",
        "\n",
        "    E_out_list.append(calc_error(w_, d_test))\n",
        "    epoch_list.append(epoch)\n",
        "\n",
        "print('E_out: ', np.sum(E_out_list)/len(E_out_list))\n",
        "print('epoch: ', np.sum(epoch_list)/len(epoch_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGmtUrtvoFNB",
        "outputId": "5c0bd0e3-9efd-420f-e45c-bc331bd50d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E_out:  0.10241205879527879\n",
            "epoch:  346.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLA as SGD"
      ],
      "metadata": {
        "id": "FPrFjDCxH947"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. (e) -min($0, y_n$**w^Tx_n**)"
      ],
      "metadata": {
        "id": "gLvvUfsXCmUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD and PLA both reduce the error based on an individual point at a time. To simulate PLA, we want the gradient of $e_n(w)$ to be 0 when classified correctly and $-yw^Tx$ when classified incorrectly. Choice **[e]** shows this."
      ],
      "metadata": {
        "id": "GbB6iKbSIorX"
      }
    }
  ]
}